{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02fe5322",
   "metadata": {},
   "source": [
    "# Dataset Loading\n",
    "The first step in any machine learning project is to load the dataset. This typically involves reading the data from a file or database and creating a data structure that can be used by the machine learning algorithms. <br> \n",
    "The dataset can be split into three sets - training set, validation set, and test set.\n",
    "\n",
    "Before loading the dataset, it is important to understand the data and its features. This can be done by analyzing the data distribution, the correlation between features, and identifying missing or invalid values. Once the dataset is loaded, it is important to split it into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6737a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71381642",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Datasets/IMDB_Urdu_Reviews/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20f9acab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>میں نے اسے 80 کی دہائی کے وسط میں ایک کیبل گائ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چونکہ میں نے 80 کی دہائی میں انسپکٹر گیجٹ کارٹ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ایک ایسے معاشرے کی حالت کے بارے میں تعجب کرتا ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>مفید البرٹ پیون کی طرف سے ایک اور ردی کی ٹوکری...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>یہ کولمبو ہے جس کی ہدایتکاری اپنے کیریئر کے اب...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  میں نے اسے 80 کی دہائی کے وسط میں ایک کیبل گائ...  positive\n",
       "1  چونکہ میں نے 80 کی دہائی میں انسپکٹر گیجٹ کارٹ...  negative\n",
       "2  ایک ایسے معاشرے کی حالت کے بارے میں تعجب کرتا ...  positive\n",
       "3  مفید البرٹ پیون کی طرف سے ایک اور ردی کی ٹوکری...  negative\n",
       "4  یہ کولمبو ہے جس کی ہدایتکاری اپنے کیریئر کے اب...  positive"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d15ad57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6eaeba9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>39737</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>آج کا شو پسند آیا !!! یہ ایک قسم تھی اور نہ صر...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "      <td>20082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                               40000     40000\n",
       "unique                                              39737         2\n",
       "top     آج کا شو پسند آیا !!! یہ ایک قسم تھی اور نہ صر...  negative\n",
       "freq                                                    4     20082"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e41ff01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "199dd7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db953643",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"labels\"] = le.fit_transform(df.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "688d90fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>میں نے اسے 80 کی دہائی کے وسط میں ایک کیبل گائ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چونکہ میں نے 80 کی دہائی میں انسپکٹر گیجٹ کارٹ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ایک ایسے معاشرے کی حالت کے بارے میں تعجب کرتا ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>مفید البرٹ پیون کی طرف سے ایک اور ردی کی ٹوکری...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>یہ کولمبو ہے جس کی ہدایتکاری اپنے کیریئر کے اب...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  labels\n",
       "0  میں نے اسے 80 کی دہائی کے وسط میں ایک کیبل گائ...  positive       1\n",
       "1  چونکہ میں نے 80 کی دہائی میں انسپکٹر گیجٹ کارٹ...  negative       0\n",
       "2  ایک ایسے معاشرے کی حالت کے بارے میں تعجب کرتا ...  positive       1\n",
       "3  مفید البرٹ پیون کی طرف سے ایک اور ردی کی ٹوکری...  negative       0\n",
       "4  یہ کولمبو ہے جس کی ہدایتکاری اپنے کیریئر کے اب...  positive       1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c3d6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urduhack\n",
    "from urduhack.preprocessing import remove_punctuation, remove_accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bd17d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review\"] = df[\"review\"].apply(lambda x: remove_punctuation(x))\n",
    "df[\"review\"] = df[\"review\"].apply(lambda x: remove_accents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f14afd39",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'urduhack' has no attribute 'tokenize_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Tokenize the reviews\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreview\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murduhack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m-> 1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1137\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [35]\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Tokenize the reviews\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43murduhack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize_words\u001b[49m(x))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'urduhack' has no attribute 'tokenize_words'"
     ]
    }
   ],
   "source": [
    "df[\"review\"] = df[\"review\"].apply(lambda x: urduhack.tokenize_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review\"] = df[\"review\"].apply(lambda x: [urduhack.stemmer.stem(w) for w in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c5fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review\"] = df[\"review\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362296f",
   "metadata": {},
   "source": [
    "# Training, Validation and Test Sets\n",
    "The training set is used to train the machine learning model. The validation set is used to fine-tune the model by adjusting hyperparameters or other settings. Finally, the test set is used to evaluate the model's performance on unseen data.\n",
    "\n",
    "<b>Training Set:</b>\n",
    "The training set is used to train the machine learning model. It typically consists of a large number of data samples, which are used to fit the model's parameters. The goal of training is to minimize the difference between the predicted output of the model and the actual output of the training set.\n",
    "\n",
    "<b>Validation Set:</b>\n",
    "The validation set is used to fine-tune the model by adjusting hyperparameters or other settings. It is also used to evaluate the model's performance during training. The validation set is typically smaller than the training set, and it is important that it is representative of the data distribution.\n",
    "\n",
    "<b>Test Set:</b>\n",
    "The test set is used to evaluate the model's performance on unseen data. It is important that the test set is completely separate from the training and validation sets to ensure that the model has not been overfitted to the training data. The test set should be large enough to provide a statistically significant evaluation of the model's performance.\n",
    "\n",
    "<b>Iterated K-Fold Validation with Shuffling:</b>\n",
    "In situations where there is relatively little data available and you need to evaluate your model as precisely as possible, iterated k-fold validation with shuffling can be used. This involves applying k-fold validation multiple times, shuffling the data every time before splitting it k ways. The final score is the average of the scores obtained at each run of k-fold validation. It is important to note that this method can be computationally expensive since"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b9eb0a",
   "metadata": {},
   "source": [
    "## Splitting the dataset into training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71d683b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a9f0529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into 80% training and 20% test sets\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the training data further into 80% training and 20% validation sets\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da48cac",
   "metadata": {},
   "source": [
    "## Alternatively, we can use the K-fold cross-validation technique to split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51ad1b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188a3122",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='urdu', max_features=5000)\n",
    "X = vectorizer.fit_transform(df.reviews)\n",
    "y = df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86fc802",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the number of folds (K)\n",
    "K = 5\n",
    "\n",
    "# Initialize the K-fold cross-validator\n",
    "kfold = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "# Split the dataset into K folds and iterate over them\n",
    "validation_scores = []\n",
    "for fold, (train_index, val_index) in enumerate(kfold.split(X)):\n",
    "    # Get the training and validation data for this fold\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    # Train an SVM model on the training data and evaluate it on the validation data\n",
    "    model = SVC(kernel='linear')\n",
    "    model.fit(X_train, y_train)\n",
    "    val_score = model.score(X_val, y_val)\n",
    "    \n",
    "    # Do something with the validation score, such as computing the average\n",
    "    validation_scores.append(val_score)\n",
    "    \n",
    "# Compute the final score as the average of the validation scores\n",
    "final_score = np.mean(validation_scores)\n",
    "print(f'Average validation score: {final_score:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
