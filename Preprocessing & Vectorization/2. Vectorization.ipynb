{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc10dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import Plotting Libararies\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import Data Preprocessing Libraries \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Model Evaluation Libraries\n",
    "from sklearn.metrics import classification_report, confusion_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d664ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../DataSets/IMDB_Urdu_Reviews/preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c327805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>encoded_sentiments</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>دہائی وسط کیبل گائیڈ اسکائینجر ہنٹ پہلو اپیل ع...</td>\n",
       "      <td>1</td>\n",
       "      <td>['دہائی', 'وسط', 'کیبل', 'گائیڈ', 'اسکائینجر',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>دہائی انسپکٹر گیجٹ کارٹون پسند فلم دیکھنے پیسہ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['دہائی', 'انسپکٹر', 'گیجٹ', 'کارٹون', 'پسند',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>معاشرے حالت تعجب والد پیدا البرٹ ٹی فٹزجیرالڈ ...</td>\n",
       "      <td>1</td>\n",
       "      <td>['معاشرے', 'حالت', 'تعجب', 'والد', 'پیدا', 'ال...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>مفید البرٹ پیون ردی ٹوکری گریڈ زیڈ جلدی ٹم تھا...</td>\n",
       "      <td>0</td>\n",
       "      <td>['مفید', 'البرٹ', 'پیون', 'ردی', 'ٹوکری', 'گری...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>کولمبو ہدایتکاری کیریئر ابتدائی وقت اسٹیون اسپ...</td>\n",
       "      <td>1</td>\n",
       "      <td>['کولمبو', 'ہدایتکاری', 'کیریئر', 'ابتدائی', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     lemmatized_text  encoded_sentiments  \\\n",
       "0  دہائی وسط کیبل گائیڈ اسکائینجر ہنٹ پہلو اپیل ع...                   1   \n",
       "1  دہائی انسپکٹر گیجٹ کارٹون پسند فلم دیکھنے پیسہ...                   0   \n",
       "2  معاشرے حالت تعجب والد پیدا البرٹ ٹی فٹزجیرالڈ ...                   1   \n",
       "3  مفید البرٹ پیون ردی ٹوکری گریڈ زیڈ جلدی ٹم تھا...                   0   \n",
       "4  کولمبو ہدایتکاری کیریئر ابتدائی وقت اسٹیون اسپ...                   1   \n",
       "\n",
       "                                              tokens  \n",
       "0  ['دہائی', 'وسط', 'کیبل', 'گائیڈ', 'اسکائینجر',...  \n",
       "1  ['دہائی', 'انسپکٹر', 'گیجٹ', 'کارٹون', 'پسند',...  \n",
       "2  ['معاشرے', 'حالت', 'تعجب', 'والد', 'پیدا', 'ال...  \n",
       "3  ['مفید', 'البرٹ', 'پیون', 'ردی', 'ٹوکری', 'گری...  \n",
       "4  ['کولمبو', 'ہدایتکاری', 'کیریئر', 'ابتدائی', '...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7bfb771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoded_sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       encoded_sentiments\n",
       "count        50000.000000\n",
       "mean             0.500000\n",
       "std              0.500005\n",
       "min              0.000000\n",
       "25%              0.000000\n",
       "50%              0.500000\n",
       "75%              1.000000\n",
       "max              1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"lemmatized_text\", \"encoded_sentiments\", \"tokens\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7449f388",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83d3f90",
   "metadata": {},
   "source": [
    "## Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d69e03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29c42d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec = Word2Vec(sentences=df[\"tokens\"], vector_size=128, window=5, workers=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c9c9a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model_word2vec.wv\n",
    "VOCAB_SIZE = len(word_vectors)\n",
    "DIMENSIONS = word_vectors.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9f7c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df.tokens)\n",
    "encoded = tokenizer.texts_to_sequences(df.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b65c0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = max(map(len, encoded))\n",
    "vectors = pad_sequences(encoded, padding='post', maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bad16530",
   "metadata": {},
   "outputs": [],
   "source": [
    "words2vec_matrix = np.zeros((VOCAB_SIZE, DIMENSIONS))\n",
    "word_index = tokenizer.word_index\n",
    "for word, index in word_index.items():\n",
    "    if word in word_vectors:\n",
    "        words2vec_matrix[index] = word_vectors[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8110dca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1401,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[746].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a0c3fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197 128 1401\n"
     ]
    }
   ],
   "source": [
    "print(VOCAB_SIZE, DIMENSIONS, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef2575d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"word2vec-VS-\"+str(VOCAB_SIZE)+\"-D-\"+str(DIMENSIONS)+\"-ML-\"+str(MAX_LEN)+\".npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a193f9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(file, sequences=vectors, labels=df.encoded_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f55413b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = np.load(file)\n",
    "X = loaded_data[\"sequences\"]\n",
    "Y = loaded_data[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "394757a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  127,   857,  1718, ...,     0,     0,     0],\n",
       "       [  127,  3077,  4125, ...,     0,     0,     0],\n",
       "       [  843,   892,  1732, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [    1,   472,   208, ...,     0,     0,     0],\n",
       "       [   68,   298,     1, ...,     0,     0,     0],\n",
       "       [15035,  3680,    12, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897b8523",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd7c7d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fc29a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fasttext.util\n",
    "# fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "# ft = fasttext.load_model('cc.en.300.bin')\n",
    "# sentences = [sentence_tokenizer(text) for text in df['Review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6d1be91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ftModel = FastText(df.tokens, vector_size=100, window=5, min_count=5, epochs=10)\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# token = Tokenizer()\n",
    "# token.fit_on_texts(df[\"tokens\"])\n",
    "# encoded = token.texts_to_sequences(df[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8854da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOCAB_SIZE = len(ftModel.wv.key_to_index)\n",
    "# DIMENSIONS = 128\n",
    "# MAX_LEN = max([len(x) for x in df[\"tokens\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11363c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_matrix = np.zeros((VOCAB_SIZE+1,DIMENSIONS))\n",
    "# for word, index in token.word_index.items():\n",
    "#     try:\n",
    "#         ft_matrix[index] = ftModel.wv[word]\n",
    "#     except:\n",
    "#         print(index, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26eef640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import preprocessing \n",
    "# train_vectors = preprocessing.sequence.pad_sequences(encoded,padding='post',dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66f5ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_data = np.load(\"word2Vec.npz\")\n",
    "# X = loaded_data[\"sequences\"]\n",
    "# Y = loaded_data[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64289bf5",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25268bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28f16ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90cc5cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Fit the vectorizer on the text data\n",
    "    tfidf_vectors = vectorizer.fit_transform(df[\"tokens\"])\n",
    "\n",
    "    # Handle any specific exception if required\n",
    "except Exception as e:\n",
    "    print(\"An error occurred during vectorization:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ac697686",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 37.9 GiB for an array with shape (50000, 101752) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [60]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_vectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py:1039\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1039\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py:1202\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 37.9 GiB for an array with shape (50000, 101752) and data type float64"
     ]
    }
   ],
   "source": [
    "arr = tfidf_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87a4bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "480cfa1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump((tfidf_vectors, df.encoded_sentiments), 'tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "41cd9a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer, labels = joblib.load('tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896088af",
   "metadata": {},
   "source": [
    "## N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "68707725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "57469d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 2))# bigram (2-gram)\n",
    "\n",
    "# Fit and transform the data\n",
    "X = vectorizer.fit_transform(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "eab3927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ngram_vectors.pkl\", \"wb\") as file:\n",
    "    pickle.dump((X, sentiment_labels), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c7fa1362",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ngram_vectors.pkl\", \"rb\") as file:\n",
    "    X_loaded, sentiment_labels = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d8ad5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8350812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
